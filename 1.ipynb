{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Data Preprocessing\n",
    "\n",
    "This section of the code fetches the CIFAR-10 dataset using the torchvision library and applies a series of transformations to preprocess the data. Here's a breakdown of the preprocessing steps:\n",
    "\n",
    "1. Data Fetching:\n",
    "\n",
    "- The CIFAR-10 dataset is downloaded and loaded using `torchvision.datasets.CIFAR10`.\n",
    "\n",
    "2. Data Transformations:\n",
    "\n",
    "- `ToTensor`: Converts images to PyTorch tensors\n",
    "- `Normalize`: Normalizes the pixel values of each image. The pixel values are scaled to a range of [-1, 1] by subtracting the mean (0.5, 0.5, 0.5) and dividing by the standard deviation (0.5, 0.5, 0.5) for each color channel (RGB).\n",
    "\n",
    "3. Dataset Splitting:\n",
    "\n",
    "- The dataset is split into three subsets: training, validation, and testing.\n",
    "- The proportions are set as 80% for training, 10% for validation, and 10% for testing.\n",
    "- The `torch.utils.data.random_split` function is used for splitting.\n",
    "\n",
    "4. Data Loaders:\n",
    "\n",
    "- `DataLoader`: Wraps the datasets into iterable loaders, allowing batches of data to be efficiently fetched during training and evaluation.\n",
    "- Parameters:\n",
    "  - `batch_size=64`: The data is processed in mini-batches of 64 images to optimize memory usage and computational efficiency.\n",
    "  - `shuffle=True` (for training): Randomly shuffles the training data to reduce model bias.\n",
    "  - `shuffle=False` (for validation and testing): Maintains the order for consistent evaluation.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "{'images': torch.Size([64, 3, 32, 32]), 'labels': torch.Size([64])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "print({'images': images.shape, 'labels': labels.shape})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: MLP Model Architecture\n",
    "\n",
    "This section defines, trains, and evaluates a simple Multi-Layer Perceptron (MLP) model for classifying CIFAR-10 images.\n",
    "\n",
    "1. **Model Definition**:\n",
    "   - The `MLPClassifier` class defines the architecture:\n",
    "     - Three fully connected layers: \n",
    "       - `fc1`: Input layer maps 3 * 32 * 32 (32 x 32 images with 3 color channels) = 3072 pixels to 512 neurons.\n",
    "       - `fc2`: Hidden layer with 128 neurons.\n",
    "       - `fc3`: Output layer with 10 neurons (for the 10 classes).\n",
    "     - ReLU activation is used after each layer, except the last one.\n",
    "\n",
    "2. **Loss and Optimizer**:\n",
    "   - Loss: `CrossEntropyLoss`, suitable for multi-class classification.\n",
    "   - Optimizer: `Adam` optimizer with a learning rate of 0.001.\n",
    "\n",
    "3. **Training Loop**:\n",
    "   - The model is trained for 25 epochs.\n",
    "   - Steps:\n",
    "     - Set the model to training mode (`model.train()`).\n",
    "     - Loop through batches of images and labels from `train_loader`.\n",
    "     - Compute predictions, loss, and gradients, and update weights.\n",
    "     - Track the running loss for monitoring.\n",
    "\n",
    "4. **Validation**:\n",
    "   - After training, the model is evaluated on the validation set (`model.eval()`).\n",
    "   - Steps:\n",
    "     - Loop through batches from `val_loader` without gradient computation (`torch.no_grad()`).\n",
    "     - Compute predictions and compare them with ground-truth labels.\n",
    "     - Calculate and print the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 1.6667456855773926\n",
      "Epoch 2, loss: 1.4602145307540892\n",
      "Epoch 3, loss: 1.3486177486419677\n",
      "Epoch 4, loss: 1.2611588106155396\n",
      "Epoch 5, loss: 1.1833021311759948\n",
      "Epoch 6, loss: 1.0962547177314759\n",
      "Epoch 7, loss: 1.0235486729621888\n",
      "Epoch 8, loss: 0.9549388641357421\n",
      "Epoch 9, loss: 0.8759211818695068\n",
      "Epoch 10, loss: 0.8144184517860412\n",
      "Epoch 11, loss: 0.7524882660388946\n",
      "Epoch 12, loss: 0.6988078862190247\n",
      "Epoch 13, loss: 0.6388286056041718\n",
      "Epoch 14, loss: 0.5990701889991761\n",
      "Epoch 15, loss: 0.5516770469903945\n",
      "Epoch 16, loss: 0.5126571859121323\n",
      "Epoch 17, loss: 0.480721221280098\n",
      "Epoch 18, loss: 0.44483970971107484\n",
      "Epoch 19, loss: 0.41846851511001587\n",
      "Epoch 20, loss: 0.3900272782087326\n",
      "Epoch 21, loss: 0.37842908816337584\n",
      "Epoch 22, loss: 0.35487020299434663\n",
      "Epoch 23, loss: 0.347210074532032\n",
      "Epoch 24, loss: 0.32436544207334517\n",
      "Epoch 25, loss: 0.3127782540082932\n",
      "Validation accuracy: 51.34\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(3*32*32, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = MLPClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Validation accuracy: {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CNN Model Architecture\n",
    "\n",
    "This section defines a Convolutional Neural Network (CNN) for classifying CIFAR-10 images. The CNN is designed to leverage convolutional layers for feature extraction.\n",
    "\n",
    "1. **Model Definition**:\n",
    "   - The `CNNClassifier` class defines the architecture:\n",
    "     - **Convolutional Layers**:\n",
    "       - `conv1`: Convolution with 32 filters, (3 * 3) kernel, stride 1, and padding 1.\n",
    "       - `conv2`: Convolution with 64 filters, (3 * 3) kernel, stride 1, and padding 1.\n",
    "       - `conv3`: Convolution with 128 filters, (3 * 3) kernel, stride 1, and padding 1.\n",
    "     - **Pooling**:\n",
    "       - Max-pooling (2 * 2), stride 2 is applied after each convolution layer to downsample the feature maps.\n",
    "     - **Fully Connected Layers**:\n",
    "       - `fc1`: Linear layer mapping 128 * 4 * 4 = 2048 features to 256.\n",
    "       - `fc2`: Output layer with 10 neurons (one for each class).\n",
    "     - Additional features:\n",
    "       - ReLU activation after each layer (except the last).\n",
    "       - Dropout (50%) in the fully connected layers to prevent overfitting.\n",
    "\n",
    "2. **Training and Evaluation**:\n",
    "   - The training and evaluation processes are the same as for the MLP classifier, utilizing the `CrossEntropyLoss` and `Adam` optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 1.5425277339935304\n",
      "Epoch 2, loss: 1.154657793045044\n",
      "Epoch 3, loss: 0.972494538974762\n",
      "Epoch 4, loss: 0.8484634868621826\n",
      "Epoch 5, loss: 0.7580944016456604\n",
      "Epoch 6, loss: 0.6799252791881562\n",
      "Epoch 7, loss: 0.6099780755519867\n",
      "Epoch 8, loss: 0.5509095715999603\n",
      "Epoch 9, loss: 0.4976423180341721\n",
      "Epoch 10, loss: 0.46005331478118894\n",
      "Epoch 11, loss: 0.4129638736486435\n",
      "Epoch 12, loss: 0.37255605022907257\n",
      "Epoch 13, loss: 0.3487615005493164\n",
      "Epoch 14, loss: 0.3183569635391235\n",
      "Epoch 15, loss: 0.29728884640932085\n",
      "Epoch 16, loss: 0.27631196895837784\n",
      "Epoch 17, loss: 0.2577621481895447\n",
      "Epoch 18, loss: 0.2396953366756439\n",
      "Epoch 19, loss: 0.2263425535529852\n",
      "Epoch 20, loss: 0.21669124541282653\n",
      "Epoch 21, loss: 0.20572140931487085\n",
      "Epoch 22, loss: 0.19801197879314422\n",
      "Epoch 23, loss: 0.18922382845282554\n",
      "Epoch 24, loss: 0.18619295721054077\n",
      "Epoch 25, loss: 0.17833629528880118\n",
      "Validation accuracy: 75.82\n"
     ]
    }
   ],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    \n",
    "    def forward(self, x : Tensor) -> Tensor:\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = CNNClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Validation accuracy: {100 * correct / total}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
